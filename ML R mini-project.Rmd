---
title: "R Notebook"
author: Steffen Lehmann - 20153945
output: word_document 
---

```{r Load packages, include=FALSE}
library(Rmisc) # summarySE
library(MASS) # polr
library(tidyverse)
library(tidymodels)
library(skimr)
library(lubridate)  # for handling dates and time
library(Hmisc)    # correlation
library(corrplot)
library(PerformanceAnalytics)
library(RColorBrewer)
library(GGally)
library(psych) # ICC
library(ggbeeswarm)
library(dplyr)
library(ggplot2)
library(reticulate) # for python
library(factoextra)
library(caret)
library(ggfortify)
library(cluster)
library(reshape2) 
```

```{python Loading packages, include=FALSE}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA,TruncatedSVD
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
import matplotlib.pyplot as plt # for plotting
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from mlxtend.plotting import plot_decision_regions
from sklearn.model_selection import cross_val_predict
```

```{r load data, include=FALSE}
data <- read.table("parkinsons.data", header=TRUE, sep=",")
```

```{r Quick overview, include=FALSE}
healthy_label <- "1"
parkinsons_label <- "0"
Healthy <- data %>% filter(status==healthy_label)
Parkinsons <- data %>% filter(status==parkinsons_label)

#I am looking at the distribution of the data and within each label. This also check the data for empty cells 
Overview <- skim(data)
Overview_Healthy <- skim(Healthy)
Overview_Parkinsons <- skim(Parkinsons)
```


```{r Correlation checks}
cordata <- data %>% dplyr::select(-name, -status)
cordata <- cor(cordata)
corrplot(cordata, order = "hclust", tl.cex = 0.7)

highlyCor <- colnames(data)[findCorrelation(cordata, cutoff = 0.9, verbose = TRUE)]

corcheck <- data[, which(!colnames(data) %in% highlyCor)] # by pair comparation

corcheck <- corcheck %>% dplyr::select(-name)

```
```{r Normality checks in R}
#Plot histograms of "_mean" variables group by diagnosis
ggplot(data = melt(corcheck, id.var = "status"), mapping = aes(x = value)) + theme_minimal() + 
    geom_histogram(bins = 10, aes(fill=as.factor(status)), alpha=0.5) + facet_wrap(~variable, scales =      'free_x') + labs(fill="status") + 
  scale_fill_discrete(labels = c("Parkinson's", "Healthy")) + theme(axis.title.x = element_blank(), axis.title.y = element_blank())
```


```{python Pre-processing in Python}
#converting the data frame from R to Python
#py_data = r.data
py_data = r.corcheck

#Pre-processing
#   dropping / removing the labels from the data frame 
X = py_data.drop(columns=['status'], axis=1)
XforPCA = py_data.drop(columns=['status'], axis=1)
#   data frame that is only containing the label
Y = py_data['status']

#Scaling the data. 
#https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler 
scaler = StandardScaler()
#Only fit on the training set
scaler.fit(X)
#Transform both sets
X = scaler.transform(X)

```

```{python PCA plot of the data in Python}
#pca with two principal components
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(XforPCA)
#first principal component contains 72.89% of the variance and the second principal component contains 21.82% of the variance. 
PCA_variance_n2 = pca.explained_variance_ratio_ 

principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2'])

#Concatenating DataFrames
plotDf = pd.concat([principalDf, Y], axis = 1)

#Scaling the data. 
scaler = StandardScaler()
scaler.fit(plotDf)
X_PCA_scaled = scaler.transform(plotDf)
```

```{r plotting 2D data}
PCA2 <- py$plotDf
scal_PCA2 <- py$X_PCA_scaled

scal_PCA2Df <- as.data.frame(scal_PCA2)

scal_PCA2Df['V3'][scal_PCA2Df['V3'] > 0] <- 1
scal_PCA2Df['V3'][scal_PCA2Df['V3'] < 0] <- 0
scal_PCA2Df <- scal_PCA2Df %>% rename(PC1 = V1, PC2 = V2, status = V3)

# Scatter plot of PCA(n=2)
ggplot(PCA2, aes(x=PC1, y=PC2, group=factor(status), colour=factor(status))) + geom_point() + stat_ellipse()
# Shows scatter of scaled dataPCA(n=2)
ggplot(scal_PCA2Df, aes(x=PC1, y=PC2, group=factor(status), colour=factor(status))) + geom_point() + stat_ellipse()

# Show the contour only PCA(n=2)
ggplot(scal_PCA2Df, aes(x=PC1, y=PC2, group=factor(status), colour=factor(status))) + geom_point() + geom_density_2d()

# Show the area only PCA(n=2)
ggplot(scal_PCA2Df, aes(x=PC1, y=PC2, group=factor(status), colour=factor(status))) + geom_point() + stat_density_2d(aes(fill = ..level..), geom="polygon")
# shows area + contour
ggplot(scal_PCA2Df, aes(x=PC1, y=PC2, group=factor(status), colour=factor(status))) + geom_point() + stat_density_2d(aes(fill = ..level..), geom="polygon", colour="white")
```
```{python PCA pipeline in Python}
# 0.95 are the components parameter. It means that scikit-learn choose the minimum number of principal components such that 95% of the variance are retained.
PCA_95 = PCA()
PCA_95.fit_transform(X)
plt.plot(np.cumsum(PCA_95.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');
plt.show()

PCAX = PCA(n_components =7)
PCAX.fit_transform(X)



Number_of_PC = PCAX.n_components_

n_components = len(PCAX.explained_variance_ratio_)

#Most important feature in per components
mostimportant = [np.abs(PCA_95.components_[i]).argmax() for i in range(n_components)]
initial_feature_names = XforPCA.columns
most_important_names = [initial_feature_names[mostimportant[i]] for i in range(n_components)]


```

```{python SVC in Python}
#Training the Support Vector classifier 
SVM_model = svm.SVC(kernel='linear')
SVM_model_scores = cross_val_score(SVM_model, X, Y, cv=5)


#Training the PCA Support Vector classifier 
SVM_PCA_model = svm.SVC(kernel='linear')
SVM_PCA_model_scores = cross_val_score(SVM_PCA_model, PCAX, Y, cv=5)


```


```{python RandomForestClassifier in Python}
#TODO model is over fitted, so the attributes of the model needs to be designed better
rf_Model = RandomForestClassifier()
rf_Model_scores = cross_val_score(rf_Model, X, Y, cv=5)


#With PCA
rf_PCA_Model = RandomForestClassifier()
rf_PCA_Model_scores = cross_val_score(rf_PCA_Model, PCAX, Y, cv=5)
```

```{python Neural Network in Python}
NN = MLPClassifier(activation = 'logistic', solver = 'sgd', hidden_layer_sizes=(100), random_state=1, max_iter=3000)
NN_scores = cross_val_score(NN, X, Y, cv=5)


NN_PCA = MLPClassifier(activation = 'logistic', solver = 'sgd', hidden_layer_sizes=(100), random_state=1, max_iter=3000)
NN_PCA_scores = cross_val_score(NN_PCA, PCAX, Y, cv=5)

```

```{python Gaussian Naive Bayes in Python}
gnb = GaussianNB()
gnb_scores = cross_val_score(gnb, X, Y, cv=5)

gnb1 = GaussianNB()
gnb1.fit(X,Y)
gnb1.class_prior_

gnb_PCA = GaussianNB()
gnb_PCA_scores = cross_val_score(gnb_PCA, PCAX, Y, cv=5)
```

```{python Model evaluation in Python}
Models = [SVM_model_scores, rf_Model_scores, NN_scores, gnb_scores]
PCA_Models= [SVM_PCA_model_scores, rf_PCA_Model_scores, NN_PCA_scores, gnb_PCA_scores]
Model_name = ['Support Vector Classifier','Random Forest Classifier', 'Neural network', 'Gaussian Naive Bayes']
AS = []
AS_PCA = []
AS_SD = []
AS_PCA_SD = []

for Model, PCA_Model in zip(Models, PCA_Models):
  AS.append(np.mean(Model)) 
  AS_SD.append(np.std(Model))
  AS_PCA.append(np.mean(PCA_Model))
  AS_PCA_SD.append(np.std(PCA_Model))

Accurracy_Store = pd.DataFrame(data=[AS,AS_SD, AS_PCA, AS_PCA_SD], columns=Model_name, index=['Standard','SD', 'PCA', 'PCA_SD'])

```

```{python Confusion Matrix}
y_pred = cross_val_predict(SVM_model, X, Y, cv=5)
cm = confusion_matrix(Y, y_pred)
SVM_CM = SVM_model.fit(X, Y)
plot_confusion_matrix(SVM_CM, X, Y)  
plt.show()
```

```{r SVM plot in R}
dat <- data.frame(y=scal_PCA2Df$status, x1=scal_PCA2Df$PC1, x2=scal_PCA2Df$PC2)
SVM_X <- scal_PCA2Df %>% dplyr::select(-status)
SVM_Y <- scal_PCA2Df$status
SVM_Y <- as.factor(SVM_Y)
s <- seq(from=-3,to=5,length=400)

# for standard SVM usage, do not set this `C` parameter so high
# this will be discussed later when we talk about "soft margin" SVM
tg <- data.frame(C=100)
fit <- train(SVM_X, SVM_Y, method="svmLinear", tuneGrid=tg)
alpha <- fit$finalModel@alpha[[1]]
sv <- as.data.frame(SVM_X[fit$finalModel@SVindex,]) # the "support vectors"
sv.y <- 2 * (as.numeric(SVM_Y[fit$finalModel@SVindex]) - 1.5)
w <- colSums(alpha * sv.y * as.matrix(sv))
b <- fit$finalModel@b
grid <- expand.grid(x1=s,x2=s)
grid$y.cont <- (as.matrix(grid[,1:2]) %*% w - b)[,1]
ggplot(dat, aes(dat$x1,dat$x2,col=dat$y)) + geom_point() + 
  geom_contour(data=grid, aes(x1,x2,z=y.cont), breaks=c(-1,0,1), col="black")

```